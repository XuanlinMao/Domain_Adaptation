{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from utils import now\n",
    "import utils\n",
    "from train import *\n",
    "from sup_models import *\n",
    "from runner import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs('./output/H2R_sup')\n",
    "# os.makedirs('./output/R2H_sup')\n",
    "# os.makedirs('./saved/models_sup_H2R')\n",
    "# os.makedirs('./saved/models_sup_R2H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = './output/R2H_sup'\n",
    "savedir = './saved/models_sup_R2H'\n",
    "# outdir = './output/H2R_sup'\n",
    "# savedir = './saved/models_sup_H2R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "utils.set_random_seeds(seed_value=123, device=device)\n",
    "\n",
    "# data_src, label_src = utils.load_data('YelpHotel', if_split=True)\n",
    "# data_tgt, label_tgt = utils.load_data('YelpRes', if_split=False)\n",
    "\n",
    "data_src, label_src = utils.load_data('YelpRes', if_split=True)\n",
    "data_tgt, label_tgt = utils.load_data('YelpHotel', if_split=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start Training ===\n",
      "Epoch: 0, Training loss: 0.7612536549568176, Val loss: 0.7612975835800171\n",
      "Epoch: 1, Training loss: 0.6935855150222778, Val loss: 0.6936196684837341\n",
      "Epoch: 2, Training loss: 0.6916784048080444, Val loss: 0.6916901469230652\n",
      "Epoch: 3, Training loss: 0.6893882751464844, Val loss: 0.689413845539093\n",
      "Epoch: 4, Training loss: 0.6855567693710327, Val loss: 0.6856062412261963\n",
      "Epoch: 5, Training loss: 0.6793872117996216, Val loss: 0.6794754266738892\n",
      "Epoch: 6, Training loss: 0.6699371337890625, Val loss: 0.6700839996337891\n",
      "Epoch: 7, Training loss: 0.6561025381088257, Val loss: 0.6563332080841064\n",
      "Epoch: 8, Training loss: 0.6366094946861267, Val loss: 0.6369528770446777\n",
      "Epoch: 9, Training loss: 0.6101709604263306, Val loss: 0.6106555461883545\n",
      "Epoch: 10, Training loss: 0.575688362121582, Val loss: 0.5763338208198547\n",
      "Epoch: 11, Training loss: 0.5326507687568665, Val loss: 0.5334531664848328\n",
      "Epoch: 12, Training loss: 0.4817102253437042, Val loss: 0.48261985182762146\n",
      "Epoch: 13, Training loss: 0.42539411783218384, Val loss: 0.42629000544548035\n",
      "Epoch: 14, Training loss: 0.3687472641468048, Val loss: 0.36942023038864136\n",
      "Epoch: 15, Training loss: 0.31932538747787476, Val loss: 0.319489985704422\n",
      "Epoch: 16, Training loss: 0.285601407289505, Val loss: 0.2849525809288025\n",
      "Epoch: 17, Training loss: 0.2732103765010834, Val loss: 0.27151381969451904\n",
      "Epoch: 18, Training loss: 0.28065869212150574, Val loss: 0.2778303921222687\n",
      "Epoch: 19, Training loss: 0.2989688515663147, Val loss: 0.2950942814350128\n",
      "Epoch: 20, Training loss: 0.317164808511734, Val loss: 0.3124534487724304\n",
      "Epoch: 21, Training loss: 0.32798516750335693, Val loss: 0.3227037787437439\n",
      "Epoch: 22, Training loss: 0.32904988527297974, Val loss: 0.32347139716148376\n",
      "Epoch: 23, Training loss: 0.32128652930259705, Val loss: 0.3156608045101166\n",
      "Epoch: 24, Training loss: 0.30725666880607605, Val loss: 0.30179616808891296\n",
      "Epoch: 25, Training loss: 0.2900943458080292, Val loss: 0.28496864438056946\n",
      "Epoch: 26, Training loss: 0.2728930413722992, Val loss: 0.2682269215583801\n",
      "Epoch: 27, Training loss: 0.25828227400779724, Val loss: 0.25415390729904175\n",
      "Epoch: 28, Training loss: 0.24804627895355225, Val loss: 0.24448512494564056\n",
      "Epoch: 29, Training loss: 0.24280093610286713, Val loss: 0.23978883028030396\n",
      "Epoch: 30, Training loss: 0.24191339313983917, Val loss: 0.23938488960266113\n",
      "Epoch: 31, Training loss: 0.2437211573123932, Val loss: 0.24157609045505524\n",
      "Epoch: 32, Training loss: 0.2462490200996399, Val loss: 0.2443719059228897\n",
      "Epoch: 33, Training loss: 0.2477380633354187, Val loss: 0.24600599706172943\n",
      "Epoch: 34, Training loss: 0.24704496562480927, Val loss: 0.24533377587795258\n",
      "Epoch: 35, Training loss: 0.24383477866649628, Val loss: 0.24202485382556915\n",
      "Epoch: 36, Training loss: 0.23842990398406982, Val loss: 0.23640713095664978\n",
      "Epoch: 37, Training loss: 0.2315206527709961, Val loss: 0.22917385399341583\n",
      "Epoch: 38, Training loss: 0.22405783832073212, Val loss: 0.221283420920372\n",
      "Epoch: 39, Training loss: 0.21696287393569946, Val loss: 0.21367147564888\n",
      "Epoch: 40, Training loss: 0.21089574694633484, Val loss: 0.20701928436756134\n",
      "Epoch: 41, Training loss: 0.20613589882850647, Val loss: 0.20163509249687195\n",
      "Epoch: 42, Training loss: 0.20254845917224884, Val loss: 0.19741737842559814\n",
      "Epoch: 43, Training loss: 0.19965554773807526, Val loss: 0.19392283260822296\n",
      "Epoch: 44, Training loss: 0.19677790999412537, Val loss: 0.1905054748058319\n",
      "Epoch: 45, Training loss: 0.19323988258838654, Val loss: 0.18652071058750153\n",
      "Epoch: 46, Training loss: 0.1886344701051712, Val loss: 0.18157529830932617\n",
      "Epoch: 47, Training loss: 0.18286901712417603, Val loss: 0.1755828857421875\n",
      "Epoch: 48, Training loss: 0.17612627148628235, Val loss: 0.16872970759868622\n",
      "Epoch: 49, Training loss: 0.16877371072769165, Val loss: 0.161377415060997\n",
      "Epoch: 50, Training loss: 0.16127479076385498, Val loss: 0.15400128066539764\n",
      "Epoch: 51, Training loss: 0.15410734713077545, Val loss: 0.14707593619823456\n",
      "Epoch: 52, Training loss: 0.14768747985363007, Val loss: 0.14101184904575348\n",
      "Epoch: 53, Training loss: 0.14221039414405823, Val loss: 0.1359543353319168\n",
      "Epoch: 54, Training loss: 0.13660509884357452, Val loss: 0.13037429749965668\n",
      "Epoch: 55, Training loss: 0.12503422796726227, Val loss: 0.11815402656793594\n",
      "Epoch: 56, Training loss: 0.11766911298036575, Val loss: 0.11125598847866058\n",
      "Epoch: 57, Training loss: 0.11266790330410004, Val loss: 0.10773685574531555\n",
      "Epoch: 58, Training loss: 0.10758781433105469, Val loss: 0.10306162387132645\n",
      "Epoch: 59, Training loss: 0.10246121883392334, Val loss: 0.09796247631311417\n",
      "Epoch: 60, Training loss: 0.0971580520272255, Val loss: 0.09322244673967361\n",
      "Epoch: 61, Training loss: 0.09257837384939194, Val loss: 0.08920782804489136\n",
      "Epoch: 62, Training loss: 0.08832532912492752, Val loss: 0.08543889224529266\n",
      "Epoch: 63, Training loss: 0.08463171869516373, Val loss: 0.08195507526397705\n",
      "Epoch: 64, Training loss: 0.08096272498369217, Val loss: 0.07951878011226654\n",
      "Epoch: 65, Training loss: 0.07751594483852386, Val loss: 0.07658660411834717\n",
      "Epoch: 66, Training loss: 0.07276610285043716, Val loss: 0.07244615256786346\n",
      "Epoch: 67, Training loss: 0.0685061439871788, Val loss: 0.06987947970628738\n",
      "Epoch: 68, Training loss: 0.06544705480337143, Val loss: 0.06877928227186203\n",
      "Epoch: 69, Training loss: 0.06358364224433899, Val loss: 0.06846188008785248\n",
      "Epoch: 70, Training loss: 0.06035077944397926, Val loss: 0.06490375846624374\n",
      "Epoch: 71, Training loss: 0.056026432663202286, Val loss: 0.059476710855960846\n",
      "Epoch: 72, Training loss: 0.05232192575931549, Val loss: 0.054040368646383286\n",
      "Epoch: 73, Training loss: 0.04950571060180664, Val loss: 0.04968978092074394\n",
      "Epoch: 74, Training loss: 0.04682150483131409, Val loss: 0.0462014377117157\n",
      "Epoch: 75, Training loss: 0.04367018863558769, Val loss: 0.042535603046417236\n",
      "Epoch: 76, Training loss: 0.04005008563399315, Val loss: 0.03919417783617973\n",
      "Epoch: 77, Training loss: 0.03732743486762047, Val loss: 0.037273697555065155\n",
      "Epoch: 78, Training loss: 0.035562124103307724, Val loss: 0.03559562563896179\n",
      "Epoch: 79, Training loss: 0.03311344236135483, Val loss: 0.032648466527462006\n",
      "Epoch: 80, Training loss: 0.03057839162647724, Val loss: 0.02953937090933323\n",
      "Epoch: 81, Training loss: 0.029131464660167694, Val loss: 0.02791045978665352\n",
      "Epoch: 82, Training loss: 0.02783077210187912, Val loss: 0.026075344532728195\n",
      "Epoch: 83, Training loss: 0.025326920673251152, Val loss: 0.023967204615473747\n",
      "Epoch: 84, Training loss: 0.02329523302614689, Val loss: 0.022130047902464867\n",
      "Epoch: 85, Training loss: 0.02204858884215355, Val loss: 0.020947283133864403\n",
      "Epoch: 86, Training loss: 0.020647825673222542, Val loss: 0.019595012068748474\n",
      "Epoch: 87, Training loss: 0.018808679655194283, Val loss: 0.01756436936557293\n",
      "Epoch: 88, Training loss: 0.01739393174648285, Val loss: 0.015855278819799423\n",
      "Epoch: 89, Training loss: 0.016373340040445328, Val loss: 0.014599603600800037\n",
      "Epoch: 90, Training loss: 0.015566544607281685, Val loss: 0.01354255061596632\n",
      "Epoch: 91, Training loss: 0.014813079498708248, Val loss: 0.012672534212470055\n",
      "Epoch: 92, Training loss: 0.014936912804841995, Val loss: 0.013957617804408073\n",
      "Epoch: 93, Training loss: 0.015560353174805641, Val loss: 0.01570628583431244\n",
      "Epoch: 94, Training loss: 0.01268180925399065, Val loss: 0.011195199564099312\n",
      "Epoch: 95, Training loss: 0.012012297287583351, Val loss: 0.007967554032802582\n",
      "Epoch: 96, Training loss: 0.013123738579452038, Val loss: 0.009177030064165592\n",
      "Epoch: 97, Training loss: 0.012994623742997646, Val loss: 0.008804754354059696\n",
      "Epoch: 98, Training loss: 0.01125299371778965, Val loss: 0.006535711232572794\n",
      "Epoch: 99, Training loss: 0.009929296560585499, Val loss: 0.007549718953669071\n",
      "Epoch: 100, Training loss: 0.009697948582470417, Val loss: 0.009665321558713913\n",
      "Epoch: 101, Training loss: 0.010529718361794949, Val loss: 0.012335700914263725\n",
      "Epoch: 102, Training loss: 0.013692956417798996, Val loss: 0.016585135832428932\n",
      "Epoch: 103, Training loss: 0.008741872385144234, Val loss: 0.009820460341870785\n",
      "Epoch: 104, Training loss: 0.008805285207927227, Val loss: 0.006406927946954966\n",
      "Epoch: 105, Training loss: 0.01046968437731266, Val loss: 0.005497663281857967\n",
      "Epoch: 106, Training loss: 0.011777129024267197, Val loss: 0.006861947011202574\n",
      "Epoch: 107, Training loss: 0.00995358545333147, Val loss: 0.0056428685784339905\n",
      "Epoch: 108, Training loss: 0.007948814891278744, Val loss: 0.0063683767803013325\n",
      "Epoch: 109, Training loss: 0.006627927999943495, Val loss: 0.008299658074975014\n",
      "Epoch: 110, Training loss: 0.039268746972084045, Val loss: 0.040532186627388\n",
      "Epoch: 111, Training loss: 0.007610732223838568, Val loss: 0.010393643751740456\n",
      "Epoch: 112, Training loss: 0.00828365795314312, Val loss: 0.005611559376120567\n",
      "Epoch: 113, Training loss: 0.015629593282938004, Val loss: 0.009733178652822971\n",
      "Epoch: 114, Training loss: 0.023801203817129135, Val loss: 0.01794453337788582\n",
      "Epoch: 115, Training loss: 0.028191881254315376, Val loss: 0.02178654447197914\n",
      "Epoch: 116, Training loss: 0.026542866602540016, Val loss: 0.02021137811243534\n",
      "Epoch: 117, Training loss: 0.020261172205209732, Val loss: 0.014206935651600361\n",
      "Epoch: 118, Training loss: 0.012545102275907993, Val loss: 0.006511239800602198\n",
      "Epoch: 119, Training loss: 0.007130618207156658, Val loss: 0.004532941617071629\n",
      "Epoch: 120, Training loss: 0.0051566967740654945, Val loss: 0.006328882649540901\n",
      "Epoch: 121, Training loss: 0.00966053456068039, Val loss: 0.012301231734454632\n",
      "Epoch: 122, Training loss: 0.015452304854989052, Val loss: 0.018139023333787918\n",
      "Epoch: 123, Training loss: 0.010208501480519772, Val loss: 0.012411263771355152\n",
      "Epoch: 124, Training loss: 0.005571428686380386, Val loss: 0.006484592333436012\n",
      "Epoch: 125, Training loss: 0.0045805019326508045, Val loss: 0.0041069332510232925\n",
      "Epoch: 126, Training loss: 0.006333582103252411, Val loss: 0.0034084711223840714\n",
      "Epoch: 127, Training loss: 0.008070625364780426, Val loss: 0.003344571217894554\n",
      "Epoch: 128, Training loss: 0.009210911579430103, Val loss: 0.0034963872749358416\n",
      "Epoch: 129, Training loss: 0.009448272176086903, Val loss: 0.0036127979401499033\n",
      "Epoch: 130, Training loss: 0.0088027473539114, Val loss: 0.003662338713183999\n",
      "Epoch: 131, Training loss: 0.0074373651295900345, Val loss: 0.0036707594990730286\n",
      "Epoch: 132, Training loss: 0.00554381450638175, Val loss: 0.0037243524566292763\n",
      "Epoch: 133, Training loss: 0.003957982640713453, Val loss: 0.003894320921972394\n",
      "Epoch: 134, Training loss: 0.0037137137260288, Val loss: 0.004402222111821175\n",
      "Epoch: 135, Training loss: 0.004283379763364792, Val loss: 0.005563891958445311\n",
      "Epoch: 136, Training loss: 0.005108895245939493, Val loss: 0.006679854821413755\n",
      "Epoch: 137, Training loss: 0.0050529781728982925, Val loss: 0.006886439397931099\n",
      "Epoch: 138, Training loss: 0.004216777626425028, Val loss: 0.0060839783400297165\n",
      "Epoch: 139, Training loss: 0.0035969081800431013, Val loss: 0.00496779615059495\n",
      "Epoch: 140, Training loss: 0.003241772297769785, Val loss: 0.004039724823087454\n",
      "Epoch: 141, Training loss: 0.0033471169881522655, Val loss: 0.0033588449005037546\n",
      "Epoch: 142, Training loss: 0.003103007562458515, Val loss: 0.003608707105740905\n",
      "Epoch: 143, Training loss: 0.0031391873490065336, Val loss: 0.0040818387642502785\n",
      "Epoch: 144, Training loss: 0.0031151683069765568, Val loss: 0.004688332788646221\n",
      "Epoch: 145, Training loss: 0.0032650588545948267, Val loss: 0.005035925656557083\n",
      "Epoch: 146, Training loss: 0.003155770245939493, Val loss: 0.004912621807307005\n",
      "=== Early Stopping at epoch 127, best loss_val = 0.003344571217894554 ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "source_encoder = GCN_Encoder(nhids=[8000,128,64],dropout=0.,with_bn=False)\n",
    "classifier = Classifier(nhids=[64,32,1],dropout=0.,with_bn=False)\n",
    "runner = SupRunner(data_src, label_src, data_tgt, label_tgt, source_encoder, classifier, device)\n",
    "runner.train(printing=True)\n",
    "runner.save('GCN',outdir,savedir)\n",
    "runner.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start Training ===\n",
      "Epoch: 0, Training loss: 0.7221831679344177, Val loss: 0.7221972942352295\n",
      "Epoch: 1, Training loss: 0.6925121545791626, Val loss: 0.6925084590911865\n",
      "Epoch: 2, Training loss: 0.6900593042373657, Val loss: 0.6900421380996704\n",
      "Epoch: 3, Training loss: 0.684831440448761, Val loss: 0.6847795844078064\n",
      "Epoch: 4, Training loss: 0.6753822565078735, Val loss: 0.6752706170082092\n",
      "Epoch: 5, Training loss: 0.659377932548523, Val loss: 0.6591473817825317\n",
      "Epoch: 6, Training loss: 0.6341628432273865, Val loss: 0.6337112188339233\n",
      "Epoch: 7, Training loss: 0.5972418785095215, Val loss: 0.5963925123214722\n",
      "Epoch: 8, Training loss: 0.5472590923309326, Val loss: 0.5457175374031067\n",
      "Epoch: 9, Training loss: 0.4854707419872284, Val loss: 0.4828384220600128\n",
      "Epoch: 10, Training loss: 0.4180004596710205, Val loss: 0.4138096868991852\n",
      "Epoch: 11, Training loss: 0.3570652902126312, Val loss: 0.3508097231388092\n",
      "Epoch: 12, Training loss: 0.31910768151283264, Val loss: 0.31043851375579834\n",
      "Epoch: 13, Training loss: 0.3142295777797699, Val loss: 0.3032998740673065\n",
      "Epoch: 14, Training loss: 0.3321695625782013, Val loss: 0.32006850838661194\n",
      "Epoch: 15, Training loss: 0.3487875163555145, Val loss: 0.3371983468532562\n",
      "Epoch: 16, Training loss: 0.3497012257575989, Val loss: 0.34014880657196045\n",
      "Epoch: 17, Training loss: 0.334347128868103, Val loss: 0.3277473747730255\n",
      "Epoch: 18, Training loss: 0.30859580636024475, Val loss: 0.3052680790424347\n",
      "Epoch: 19, Training loss: 0.28007182478904724, Val loss: 0.2798633575439453\n",
      "Epoch: 20, Training loss: 0.2558554708957672, Val loss: 0.258332222700119\n",
      "Epoch: 21, Training loss: 0.2410249412059784, Val loss: 0.24560198187828064\n",
      "Epoch: 22, Training loss: 0.23654787242412567, Val loss: 0.24264207482337952\n",
      "Epoch: 23, Training loss: 0.23815257847309113, Val loss: 0.24517370760440826\n",
      "Epoch: 24, Training loss: 0.23800736665725708, Val loss: 0.24589583277702332\n",
      "Epoch: 25, Training loss: 0.23149704933166504, Val loss: 0.23982568085193634\n",
      "Epoch: 26, Training loss: 0.21701779961585999, Val loss: 0.22680795192718506\n",
      "Epoch: 27, Training loss: 0.19873148202896118, Val loss: 0.21003179252147675\n",
      "Epoch: 28, Training loss: 0.18099692463874817, Val loss: 0.19392389059066772\n",
      "Epoch: 29, Training loss: 0.16610100865364075, Val loss: 0.18083909153938293\n",
      "Epoch: 30, Training loss: 0.15438082814216614, Val loss: 0.1712387502193451\n",
      "Epoch: 31, Training loss: 0.14479976892471313, Val loss: 0.16335344314575195\n",
      "Epoch: 32, Training loss: 0.13462583720684052, Val loss: 0.15474392473697662\n",
      "Epoch: 33, Training loss: 0.12301819026470184, Val loss: 0.14382268488407135\n",
      "Epoch: 34, Training loss: 0.10871458053588867, Val loss: 0.1292959302663803\n",
      "Epoch: 35, Training loss: 0.09342913329601288, Val loss: 0.11381056159734726\n",
      "Epoch: 36, Training loss: 0.07753235101699829, Val loss: 0.09669870883226395\n",
      "Epoch: 37, Training loss: 0.06172959879040718, Val loss: 0.07598646730184555\n",
      "Epoch: 38, Training loss: 0.05062202364206314, Val loss: 0.06148630380630493\n",
      "Epoch: 39, Training loss: 0.038957007229328156, Val loss: 0.048755764961242676\n",
      "Epoch: 40, Training loss: 0.03264932334423065, Val loss: 0.24204425513744354\n",
      "Epoch: 41, Training loss: 0.027162130922079086, Val loss: 0.24045279622077942\n",
      "Epoch: 42, Training loss: 0.0216384157538414, Val loss: 0.23602953553199768\n",
      "Epoch: 43, Training loss: 0.017023909837007523, Val loss: 0.12656214833259583\n",
      "Epoch: 44, Training loss: 0.014165304601192474, Val loss: 0.122322678565979\n",
      "Epoch: 45, Training loss: 0.012111370451748371, Val loss: 0.11991195380687714\n",
      "Epoch: 46, Training loss: 0.010661286301910877, Val loss: 0.11841962486505508\n",
      "Epoch: 47, Training loss: 0.009178690612316132, Val loss: 0.11795065551996231\n",
      "Epoch: 48, Training loss: 0.008194344118237495, Val loss: 0.1165580302476883\n",
      "Epoch: 49, Training loss: 0.007347805891185999, Val loss: 0.11580125242471695\n",
      "Epoch: 50, Training loss: 0.006825379561632872, Val loss: 0.11450821161270142\n",
      "Epoch: 51, Training loss: 0.006333081983029842, Val loss: 0.11361965537071228\n",
      "Epoch: 52, Training loss: 0.005631926469504833, Val loss: 0.11212608218193054\n",
      "Epoch: 53, Training loss: 0.004804943222552538, Val loss: 0.11121812462806702\n",
      "Epoch: 54, Training loss: 0.004257820546627045, Val loss: 0.10974240303039551\n",
      "Epoch: 55, Training loss: 0.0037577454932034016, Val loss: 0.10839707404375076\n",
      "Epoch: 56, Training loss: 0.0035320171155035496, Val loss: 0.10782188922166824\n",
      "Epoch: 57, Training loss: 0.0031377547420561314, Val loss: 0.1067008376121521\n",
      "Epoch: 58, Training loss: 0.002988500753417611, Val loss: 0.10510098189115524\n",
      "=== Early Stopping at epoch 39, best loss_val = 0.048755764961242676 ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "source_encoder = GraphSAGE_Encoder(nhids=[8000,128,64],dropout=0.,with_bn=False)\n",
    "classifier = Classifier(nhids=[64,32,1],dropout=0.,with_bn=False)\n",
    "runner = SupRunner(data_src, label_src, data_tgt, label_tgt, source_encoder, classifier, device)\n",
    "runner.train(printing=True)\n",
    "runner.save('GraphSAGE',outdir,savedir)\n",
    "runner.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start Training ===\n",
      "Epoch: 0, Training loss: 0.7080137133598328, Val loss: 0.7080461382865906\n",
      "Epoch: 1, Training loss: 0.6924420595169067, Val loss: 0.6924472451210022\n",
      "Epoch: 2, Training loss: 0.6910275816917419, Val loss: 0.6910423040390015\n",
      "Epoch: 3, Training loss: 0.6883828043937683, Val loss: 0.6884148716926575\n",
      "Epoch: 4, Training loss: 0.6835081577301025, Val loss: 0.6835718154907227\n",
      "Epoch: 5, Training loss: 0.6753207445144653, Val loss: 0.6754344701766968\n",
      "Epoch: 6, Training loss: 0.6623772978782654, Val loss: 0.6625627875328064\n",
      "Epoch: 7, Training loss: 0.6429125666618347, Val loss: 0.6431879997253418\n",
      "Epoch: 8, Training loss: 0.6149778962135315, Val loss: 0.6153411865234375\n",
      "Epoch: 9, Training loss: 0.5767772197723389, Val loss: 0.5771742463111877\n",
      "Epoch: 10, Training loss: 0.5273848176002502, Val loss: 0.5276578664779663\n",
      "Epoch: 11, Training loss: 0.4678370952606201, Val loss: 0.4676622748374939\n",
      "Epoch: 12, Training loss: 0.40244048833847046, Val loss: 0.4012722373008728\n",
      "Epoch: 13, Training loss: 0.3397715985774994, Val loss: 0.336825430393219\n",
      "Epoch: 14, Training loss: 0.2919186055660248, Val loss: 0.28626224398612976\n",
      "Epoch: 15, Training loss: 0.26995518803596497, Val loss: 0.26077672839164734\n",
      "Epoch: 16, Training loss: 0.27603989839553833, Val loss: 0.2630273997783661\n",
      "Epoch: 17, Training loss: 0.29941123723983765, Val loss: 0.282993882894516\n",
      "Epoch: 18, Training loss: 0.32389402389526367, Val loss: 0.30511215329170227\n",
      "Epoch: 19, Training loss: 0.338602215051651, Val loss: 0.31872880458831787\n",
      "Epoch: 20, Training loss: 0.3403502106666565, Val loss: 0.3205728828907013\n",
      "Epoch: 21, Training loss: 0.33083149790763855, Val loss: 0.3120967149734497\n",
      "Epoch: 22, Training loss: 0.3138901889324188, Val loss: 0.29686349630355835\n",
      "Epoch: 23, Training loss: 0.2939697504043579, Val loss: 0.27905336022377014\n",
      "Epoch: 24, Training loss: 0.27527114748954773, Val loss: 0.2626410722732544\n",
      "Epoch: 25, Training loss: 0.26113447546958923, Val loss: 0.2507777512073517\n",
      "Epoch: 26, Training loss: 0.25343620777130127, Val loss: 0.24518534541130066\n",
      "Epoch: 27, Training loss: 0.2521532475948334, Val loss: 0.24572448432445526\n",
      "Epoch: 28, Training loss: 0.2554720640182495, Val loss: 0.2505142092704773\n",
      "Epoch: 29, Training loss: 0.26059502363204956, Val loss: 0.2567426562309265\n",
      "Epoch: 30, Training loss: 0.2648361623287201, Val loss: 0.2617509663105011\n",
      "Epoch: 31, Training loss: 0.26640287041664124, Val loss: 0.26379314064979553\n",
      "Epoch: 32, Training loss: 0.2646356523036957, Val loss: 0.2622557282447815\n",
      "Epoch: 33, Training loss: 0.2598631680011749, Val loss: 0.25750595331192017\n",
      "Epoch: 34, Training loss: 0.2531055510044098, Val loss: 0.2505965232849121\n",
      "Epoch: 35, Training loss: 0.24572885036468506, Val loss: 0.24292640388011932\n",
      "Epoch: 36, Training loss: 0.23907586932182312, Val loss: 0.23587901890277863\n",
      "Epoch: 37, Training loss: 0.2341184765100479, Val loss: 0.2304784059524536\n",
      "Epoch: 38, Training loss: 0.23122161626815796, Val loss: 0.22715052962303162\n",
      "Epoch: 39, Training loss: 0.23010385036468506, Val loss: 0.22567744553089142\n",
      "Epoch: 40, Training loss: 0.23001176118850708, Val loss: 0.22536210715770721\n",
      "Epoch: 41, Training loss: 0.2300254851579666, Val loss: 0.2253260612487793\n",
      "Epoch: 42, Training loss: 0.22937341034412384, Val loss: 0.22481422126293182\n",
      "Epoch: 43, Training loss: 0.2276313751935959, Val loss: 0.22339598834514618\n",
      "Epoch: 44, Training loss: 0.22478333115577698, Val loss: 0.22102618217468262\n",
      "Epoch: 45, Training loss: 0.22115784883499146, Val loss: 0.21798710525035858\n",
      "Epoch: 46, Training loss: 0.21723529696464539, Val loss: 0.21472159028053284\n",
      "Epoch: 47, Training loss: 0.2134963423013687, Val loss: 0.21168462932109833\n",
      "Epoch: 48, Training loss: 0.21031662821769714, Val loss: 0.209218829870224\n",
      "Epoch: 49, Training loss: 0.20787684619426727, Val loss: 0.20747096836566925\n",
      "Epoch: 50, Training loss: 0.20611776411533356, Val loss: 0.2063489854335785\n",
      "Epoch: 51, Training loss: 0.2047853022813797, Val loss: 0.20558249950408936\n",
      "Epoch: 52, Training loss: 0.203555628657341, Val loss: 0.20483440160751343\n",
      "Epoch: 53, Training loss: 0.2021404653787613, Val loss: 0.20381486415863037\n",
      "Epoch: 54, Training loss: 0.20038269460201263, Val loss: 0.20237445831298828\n",
      "Epoch: 55, Training loss: 0.1982908397912979, Val loss: 0.2005288153886795\n",
      "Epoch: 56, Training loss: 0.19599959254264832, Val loss: 0.19841784238815308\n",
      "Epoch: 57, Training loss: 0.19369405508041382, Val loss: 0.19624626636505127\n",
      "Epoch: 58, Training loss: 0.19154798984527588, Val loss: 0.19421590864658356\n",
      "Epoch: 59, Training loss: 0.1896764039993286, Val loss: 0.1924593448638916\n",
      "Epoch: 60, Training loss: 0.18811967968940735, Val loss: 0.19101998209953308\n",
      "Epoch: 61, Training loss: 0.18681640923023224, Val loss: 0.18984824419021606\n",
      "Epoch: 62, Training loss: 0.1856631338596344, Val loss: 0.18882906436920166\n",
      "Epoch: 63, Training loss: 0.1845483034849167, Val loss: 0.187841534614563\n",
      "Epoch: 64, Training loss: 0.1834074705839157, Val loss: 0.18680156767368317\n",
      "Epoch: 65, Training loss: 0.18223854899406433, Val loss: 0.18568460643291473\n",
      "Epoch: 66, Training loss: 0.18110746145248413, Val loss: 0.18452143669128418\n",
      "Epoch: 67, Training loss: 0.180045023560524, Val loss: 0.18335802853107452\n",
      "Epoch: 68, Training loss: 0.17910398542881012, Val loss: 0.18223899602890015\n",
      "Epoch: 69, Training loss: 0.1783202439546585, Val loss: 0.18121172487735748\n",
      "Epoch: 70, Training loss: 0.17771247029304504, Val loss: 0.1803065836429596\n",
      "Epoch: 71, Training loss: 0.17725272476673126, Val loss: 0.17953763902187347\n",
      "Epoch: 72, Training loss: 0.17692600190639496, Val loss: 0.1788809448480606\n",
      "Epoch: 73, Training loss: 0.17667418718338013, Val loss: 0.17827796936035156\n",
      "Epoch: 74, Training loss: 0.1764153689146042, Val loss: 0.17767620086669922\n",
      "Epoch: 75, Training loss: 0.17613109946250916, Val loss: 0.17704886198043823\n",
      "Epoch: 76, Training loss: 0.17583000659942627, Val loss: 0.1763990819454193\n",
      "Epoch: 77, Training loss: 0.17552360892295837, Val loss: 0.17574337124824524\n",
      "Epoch: 78, Training loss: 0.1752302497625351, Val loss: 0.1751009076833725\n",
      "Epoch: 79, Training loss: 0.17496414482593536, Val loss: 0.17448432743549347\n",
      "Epoch: 80, Training loss: 0.17473110556602478, Val loss: 0.17389726638793945\n",
      "Epoch: 81, Training loss: 0.17452681064605713, Val loss: 0.17333325743675232\n",
      "Epoch: 82, Training loss: 0.17433708906173706, Val loss: 0.17278002202510834\n",
      "Epoch: 83, Training loss: 0.17414727807044983, Val loss: 0.17222420871257782\n",
      "Epoch: 84, Training loss: 0.17394857108592987, Val loss: 0.17166122794151306\n",
      "Epoch: 85, Training loss: 0.17374026775360107, Val loss: 0.17111711204051971\n",
      "Epoch: 86, Training loss: 0.17353639006614685, Val loss: 0.17070209980010986\n",
      "Epoch: 87, Training loss: 0.17334170639514923, Val loss: 0.17039115726947784\n",
      "Epoch: 88, Training loss: 0.17316173017024994, Val loss: 0.17011937499046326\n",
      "Epoch: 89, Training loss: 0.1729956567287445, Val loss: 0.16986414790153503\n",
      "Epoch: 90, Training loss: 0.1728392392396927, Val loss: 0.16962192952632904\n",
      "Epoch: 91, Training loss: 0.1726911962032318, Val loss: 0.169411301612854\n",
      "Epoch: 92, Training loss: 0.17254818975925446, Val loss: 0.16923114657402039\n",
      "Epoch: 93, Training loss: 0.17240367829799652, Val loss: 0.16904522478580475\n",
      "Epoch: 94, Training loss: 0.17225687205791473, Val loss: 0.16885890066623688\n",
      "Epoch: 95, Training loss: 0.17210976779460907, Val loss: 0.16868656873703003\n",
      "Epoch: 96, Training loss: 0.17196328938007355, Val loss: 0.16852867603302002\n",
      "Epoch: 97, Training loss: 0.17181751132011414, Val loss: 0.16837066411972046\n",
      "Epoch: 98, Training loss: 0.17167377471923828, Val loss: 0.16821178793907166\n",
      "Epoch: 99, Training loss: 0.17153270542621613, Val loss: 0.16805653274059296\n",
      "Epoch: 100, Training loss: 0.17139360308647156, Val loss: 0.16790790855884552\n",
      "Epoch: 101, Training loss: 0.1712547242641449, Val loss: 0.16776743531227112\n",
      "Epoch: 102, Training loss: 0.17111420631408691, Val loss: 0.16763515770435333\n",
      "Epoch: 103, Training loss: 0.17097097635269165, Val loss: 0.16751079261302948\n",
      "Epoch: 104, Training loss: 0.17082495987415314, Val loss: 0.16739389300346375\n",
      "Epoch: 105, Training loss: 0.17067694664001465, Val loss: 0.16728371381759644\n",
      "Epoch: 106, Training loss: 0.17052830755710602, Val loss: 0.167179137468338\n",
      "Epoch: 107, Training loss: 0.17038017511367798, Val loss: 0.16707852482795715\n",
      "Epoch: 108, Training loss: 0.17023305594921112, Val loss: 0.16697938740253448\n",
      "Epoch: 109, Training loss: 0.17008644342422485, Val loss: 0.16687825322151184\n",
      "Epoch: 110, Training loss: 0.16993944346904755, Val loss: 0.16677187383174896\n",
      "Epoch: 111, Training loss: 0.1697908192873001, Val loss: 0.16665710508823395\n",
      "Epoch: 112, Training loss: 0.1696399599313736, Val loss: 0.16653279960155487\n",
      "Epoch: 113, Training loss: 0.16948696970939636, Val loss: 0.166399747133255\n",
      "Epoch: 114, Training loss: 0.16933244466781616, Val loss: 0.1662600040435791\n",
      "Epoch: 115, Training loss: 0.16917699575424194, Val loss: 0.16611626744270325\n",
      "Epoch: 116, Training loss: 0.1690210998058319, Val loss: 0.16597150266170502\n",
      "Epoch: 117, Training loss: 0.16886499524116516, Val loss: 0.16582824289798737\n",
      "Epoch: 118, Training loss: 0.16870857775211334, Val loss: 0.16568832099437714\n",
      "Epoch: 119, Training loss: 0.16855131089687347, Val loss: 0.16555273532867432\n",
      "Epoch: 120, Training loss: 0.16839270293712616, Val loss: 0.16542178392410278\n",
      "Epoch: 121, Training loss: 0.16823256015777588, Val loss: 0.16529525816440582\n",
      "Epoch: 122, Training loss: 0.16807103157043457, Val loss: 0.1651725322008133\n",
      "Epoch: 123, Training loss: 0.16790856420993805, Val loss: 0.16505248844623566\n",
      "Epoch: 124, Training loss: 0.167745903134346, Val loss: 0.1649337112903595\n",
      "Epoch: 125, Training loss: 0.167583167552948, Val loss: 0.16481421887874603\n",
      "Epoch: 126, Training loss: 0.16741995513439178, Val loss: 0.16469192504882812\n",
      "Epoch: 127, Training loss: 0.16725607216358185, Val loss: 0.16456517577171326\n",
      "Epoch: 128, Training loss: 0.16709139943122864, Val loss: 0.16443321108818054\n",
      "Epoch: 129, Training loss: 0.166925847530365, Val loss: 0.1642964631319046\n",
      "Epoch: 130, Training loss: 0.16675962507724762, Val loss: 0.16415584087371826\n",
      "Epoch: 131, Training loss: 0.16659298539161682, Val loss: 0.16401289403438568\n",
      "Epoch: 132, Training loss: 0.16642633080482483, Val loss: 0.16386936604976654\n",
      "Epoch: 133, Training loss: 0.16625989973545074, Val loss: 0.16372692584991455\n",
      "Epoch: 134, Training loss: 0.16609399020671844, Val loss: 0.1635870784521103\n",
      "Epoch: 135, Training loss: 0.1659286767244339, Val loss: 0.16345049440860748\n",
      "Epoch: 136, Training loss: 0.16576388478279114, Val loss: 0.16331744194030762\n",
      "Epoch: 137, Training loss: 0.16559943556785583, Val loss: 0.16318769752979279\n",
      "Epoch: 138, Training loss: 0.16543544828891754, Val loss: 0.16306072473526\n",
      "Epoch: 139, Training loss: 0.16527220606803894, Val loss: 0.16293592751026154\n",
      "Epoch: 140, Training loss: 0.16511008143424988, Val loss: 0.16281206905841827\n",
      "Epoch: 141, Training loss: 0.1649494767189026, Val loss: 0.16268810629844666\n",
      "Epoch: 142, Training loss: 0.16479095816612244, Val loss: 0.16256305575370789\n",
      "Epoch: 143, Training loss: 0.1646343618631363, Val loss: 0.16243678331375122\n",
      "Epoch: 144, Training loss: 0.16447965800762177, Val loss: 0.16230954229831696\n",
      "Epoch: 145, Training loss: 0.16432669758796692, Val loss: 0.16218194365501404\n",
      "Epoch: 146, Training loss: 0.16417519748210907, Val loss: 0.1620549112558365\n",
      "Epoch: 147, Training loss: 0.16402487456798553, Val loss: 0.1619296669960022\n",
      "Epoch: 148, Training loss: 0.1638752967119217, Val loss: 0.16180729866027832\n",
      "Epoch: 149, Training loss: 0.1637263149023056, Val loss: 0.16168835759162903\n",
      "Epoch: 150, Training loss: 0.163578063249588, Val loss: 0.1615728884935379\n",
      "Epoch: 151, Training loss: 0.16343064606189728, Val loss: 0.16146081686019897\n",
      "Epoch: 152, Training loss: 0.1632842719554901, Val loss: 0.16135135293006897\n",
      "Epoch: 153, Training loss: 0.1631392538547516, Val loss: 0.16124360263347626\n",
      "Epoch: 154, Training loss: 0.16299578547477722, Val loss: 0.1611364632844925\n",
      "Epoch: 155, Training loss: 0.16285386681556702, Val loss: 0.16102910041809082\n",
      "Epoch: 156, Training loss: 0.16271360218524933, Val loss: 0.16092084348201752\n",
      "Epoch: 157, Training loss: 0.1625748872756958, Val loss: 0.1608118861913681\n",
      "Epoch: 158, Training loss: 0.16243766248226166, Val loss: 0.16070275008678436\n",
      "Epoch: 159, Training loss: 0.16230204701423645, Val loss: 0.16059385240077972\n",
      "Epoch: 160, Training loss: 0.16216781735420227, Val loss: 0.16048607230186462\n",
      "Epoch: 161, Training loss: 0.16203506290912628, Val loss: 0.1603802740573883\n",
      "Epoch: 162, Training loss: 0.16190342605113983, Val loss: 0.16027750074863434\n",
      "Epoch: 163, Training loss: 0.16177304089069366, Val loss: 0.16017821431159973\n",
      "Epoch: 164, Training loss: 0.16164378821849823, Val loss: 0.16008242964744568\n",
      "Epoch: 165, Training loss: 0.16151580214500427, Val loss: 0.15998971462249756\n",
      "Epoch: 166, Training loss: 0.1613890528678894, Val loss: 0.15989947319030762\n",
      "Epoch: 167, Training loss: 0.16126342117786407, Val loss: 0.159810870885849\n",
      "Epoch: 168, Training loss: 0.16113890707492828, Val loss: 0.15972299873828888\n",
      "Epoch: 169, Training loss: 0.16101540625095367, Val loss: 0.15963539481163025\n",
      "Epoch: 170, Training loss: 0.1608927845954895, Val loss: 0.15954816341400146\n",
      "Epoch: 171, Training loss: 0.16077113151550293, Val loss: 0.1594613939523697\n",
      "Epoch: 172, Training loss: 0.1606505662202835, Val loss: 0.15937559306621552\n",
      "Epoch: 173, Training loss: 0.16053111851215363, Val loss: 0.159291073679924\n",
      "Epoch: 174, Training loss: 0.16041283309459686, Val loss: 0.15920835733413696\n",
      "Epoch: 175, Training loss: 0.16029565036296844, Val loss: 0.15912778675556183\n",
      "Epoch: 176, Training loss: 0.1601797491312027, Val loss: 0.1590496450662613\n",
      "Epoch: 177, Training loss: 0.1600649654865265, Val loss: 0.1589738428592682\n",
      "Epoch: 178, Training loss: 0.15995126962661743, Val loss: 0.15890055894851685\n",
      "Epoch: 179, Training loss: 0.15983852744102478, Val loss: 0.15882956981658936\n",
      "Epoch: 180, Training loss: 0.15972672402858734, Val loss: 0.1587604433298111\n",
      "Epoch: 181, Training loss: 0.15961572527885437, Val loss: 0.15869256854057312\n",
      "Epoch: 182, Training loss: 0.1595057100057602, Val loss: 0.15862557291984558\n",
      "Epoch: 183, Training loss: 0.1593969166278839, Val loss: 0.15855897963047028\n",
      "Epoch: 184, Training loss: 0.1592893898487091, Val loss: 0.15849238634109497\n",
      "Epoch: 185, Training loss: 0.1591830998659134, Val loss: 0.15842574834823608\n",
      "Epoch: 186, Training loss: 0.1590779572725296, Val loss: 0.1583593487739563\n",
      "Epoch: 187, Training loss: 0.1589738428592682, Val loss: 0.1582939475774765\n",
      "Epoch: 188, Training loss: 0.15887054800987244, Val loss: 0.1582302451133728\n",
      "Epoch: 189, Training loss: 0.15876805782318115, Val loss: 0.1581687033176422\n",
      "Epoch: 190, Training loss: 0.15866602957248688, Val loss: 0.1581096649169922\n",
      "Epoch: 191, Training loss: 0.15856431424617767, Val loss: 0.1580531746149063\n",
      "Epoch: 192, Training loss: 0.15846294164657593, Val loss: 0.15799877047538757\n",
      "Epoch: 193, Training loss: 0.15836192667484283, Val loss: 0.15794621407985687\n",
      "Epoch: 194, Training loss: 0.15826135873794556, Val loss: 0.15789468586444855\n",
      "Epoch: 195, Training loss: 0.1581612229347229, Val loss: 0.15784338116645813\n",
      "Epoch: 196, Training loss: 0.1580614447593689, Val loss: 0.15779158473014832\n",
      "Epoch: 197, Training loss: 0.15796200931072235, Val loss: 0.15773887932300568\n",
      "Epoch: 198, Training loss: 0.1578627973794937, Val loss: 0.1576855331659317\n",
      "Epoch: 199, Training loss: 0.1577637493610382, Val loss: 0.1576320379972458\n",
      "Epoch: 200, Training loss: 0.15766489505767822, Val loss: 0.15757884085178375\n",
      "Epoch: 201, Training loss: 0.15756626427173615, Val loss: 0.1575264185667038\n",
      "Epoch: 202, Training loss: 0.1574678122997284, Val loss: 0.15747524797916412\n",
      "Epoch: 203, Training loss: 0.15736956894397736, Val loss: 0.1574254035949707\n",
      "Epoch: 204, Training loss: 0.15727145969867706, Val loss: 0.1573769897222519\n",
      "Epoch: 205, Training loss: 0.15717355906963348, Val loss: 0.157329723238945\n",
      "Epoch: 206, Training loss: 0.15707595646381378, Val loss: 0.15728308260440826\n",
      "Epoch: 207, Training loss: 0.15697844326496124, Val loss: 0.15723693370819092\n",
      "Epoch: 208, Training loss: 0.15688098967075348, Val loss: 0.15719105303287506\n",
      "Epoch: 209, Training loss: 0.1567835509777069, Val loss: 0.1571454256772995\n",
      "Epoch: 210, Training loss: 0.15668632090091705, Val loss: 0.15709997713565826\n",
      "Epoch: 211, Training loss: 0.15658943355083466, Val loss: 0.1570548117160797\n",
      "Epoch: 212, Training loss: 0.15649276971817017, Val loss: 0.15700986981391907\n",
      "Epoch: 213, Training loss: 0.15639628469944, Val loss: 0.1569651961326599\n",
      "Epoch: 214, Training loss: 0.15629996359348297, Val loss: 0.15692095458507538\n",
      "Epoch: 215, Training loss: 0.1562037467956543, Val loss: 0.1568772941827774\n",
      "Epoch: 216, Training loss: 0.15610773861408234, Val loss: 0.15683452785015106\n",
      "Epoch: 217, Training loss: 0.1560119092464447, Val loss: 0.15679258108139038\n",
      "Epoch: 218, Training loss: 0.15591640770435333, Val loss: 0.1567513644695282\n",
      "Epoch: 219, Training loss: 0.1558215320110321, Val loss: 0.15671080350875854\n",
      "Epoch: 220, Training loss: 0.1557272970676422, Val loss: 0.1566704958677292\n",
      "Epoch: 221, Training loss: 0.15563338994979858, Val loss: 0.15663033723831177\n",
      "Epoch: 222, Training loss: 0.1555396169424057, Val loss: 0.15659062564373016\n",
      "Epoch: 223, Training loss: 0.15544593334197998, Val loss: 0.156551331281662\n",
      "Epoch: 224, Training loss: 0.15535221993923187, Val loss: 0.1565125435590744\n",
      "Epoch: 225, Training loss: 0.15525852143764496, Val loss: 0.1564740687608719\n",
      "Epoch: 226, Training loss: 0.1551647037267685, Val loss: 0.15643590688705444\n",
      "Epoch: 227, Training loss: 0.15507067739963531, Val loss: 0.15639793872833252\n",
      "Epoch: 228, Training loss: 0.15497645735740662, Val loss: 0.1563599854707718\n",
      "Epoch: 229, Training loss: 0.15488195419311523, Val loss: 0.15632200241088867\n",
      "Epoch: 230, Training loss: 0.15478716790676117, Val loss: 0.15628407895565033\n",
      "Epoch: 231, Training loss: 0.15469205379486084, Val loss: 0.15624623000621796\n",
      "Epoch: 232, Training loss: 0.15459665656089783, Val loss: 0.15620826184749603\n",
      "Epoch: 233, Training loss: 0.15450100600719452, Val loss: 0.15617018938064575\n",
      "Epoch: 234, Training loss: 0.15440498292446136, Val loss: 0.15613201260566711\n",
      "Epoch: 235, Training loss: 0.15430854260921478, Val loss: 0.15609382092952728\n",
      "Epoch: 236, Training loss: 0.15421168506145477, Val loss: 0.15605562925338745\n",
      "Epoch: 237, Training loss: 0.15411420166492462, Val loss: 0.15601791441440582\n",
      "Epoch: 238, Training loss: 0.15401610732078552, Val loss: 0.15598048269748688\n",
      "Epoch: 239, Training loss: 0.15391738712787628, Val loss: 0.15594327449798584\n",
      "Epoch: 240, Training loss: 0.1538180410861969, Val loss: 0.15590599179267883\n",
      "Epoch: 241, Training loss: 0.15371809899806976, Val loss: 0.15586844086647034\n",
      "Epoch: 242, Training loss: 0.15361760556697845, Val loss: 0.15583030879497528\n",
      "Epoch: 243, Training loss: 0.15351653099060059, Val loss: 0.15579161047935486\n",
      "Epoch: 244, Training loss: 0.15341484546661377, Val loss: 0.15575234591960907\n",
      "Epoch: 245, Training loss: 0.1533125638961792, Val loss: 0.1557125598192215\n",
      "Epoch: 246, Training loss: 0.1532096266746521, Val loss: 0.15567238628864288\n",
      "Epoch: 247, Training loss: 0.15310609340667725, Val loss: 0.15563185513019562\n",
      "Epoch: 248, Training loss: 0.15300200879573822, Val loss: 0.1555907279253006\n",
      "Epoch: 249, Training loss: 0.15289734303951263, Val loss: 0.15554875135421753\n",
      "Epoch: 250, Training loss: 0.15279211103916168, Val loss: 0.15550605952739716\n",
      "Epoch: 251, Training loss: 0.152686208486557, Val loss: 0.15546302497386932\n",
      "Epoch: 252, Training loss: 0.15257960557937622, Val loss: 0.15541993081569672\n",
      "Epoch: 253, Training loss: 0.15247215330600739, Val loss: 0.15537680685520172\n",
      "Epoch: 254, Training loss: 0.15236394107341766, Val loss: 0.15533332526683807\n",
      "Epoch: 255, Training loss: 0.15225498378276825, Val loss: 0.15528911352157593\n",
      "Epoch: 256, Training loss: 0.15214522182941437, Val loss: 0.15524399280548096\n",
      "Epoch: 257, Training loss: 0.15203455090522766, Val loss: 0.155197873711586\n",
      "Epoch: 258, Training loss: 0.1519230455160141, Val loss: 0.15515048801898956\n",
      "Epoch: 259, Training loss: 0.15181070566177368, Val loss: 0.1551022082567215\n",
      "Epoch: 260, Training loss: 0.15169748663902283, Val loss: 0.15505290031433105\n",
      "Epoch: 261, Training loss: 0.15158334374427795, Val loss: 0.15500299632549286\n",
      "Epoch: 262, Training loss: 0.15146827697753906, Val loss: 0.1549527943134308\n",
      "Epoch: 263, Training loss: 0.15135224163532257, Val loss: 0.15490247309207916\n",
      "Epoch: 264, Training loss: 0.15123522281646729, Val loss: 0.15485160052776337\n",
      "Epoch: 265, Training loss: 0.15111707150936127, Val loss: 0.1548001766204834\n",
      "Epoch: 266, Training loss: 0.15099787712097168, Val loss: 0.1547478884458542\n",
      "Epoch: 267, Training loss: 0.15087750554084778, Val loss: 0.1546948254108429\n",
      "Epoch: 268, Training loss: 0.15075601637363434, Val loss: 0.15464089810848236\n",
      "Epoch: 269, Training loss: 0.15063335001468658, Val loss: 0.1545860767364502\n",
      "Epoch: 270, Training loss: 0.15050950646400452, Val loss: 0.15453045070171356\n",
      "Epoch: 271, Training loss: 0.15038447082042694, Val loss: 0.15447403490543365\n",
      "Epoch: 272, Training loss: 0.15025828778743744, Val loss: 0.15441696345806122\n",
      "Epoch: 273, Training loss: 0.1501309722661972, Val loss: 0.1543588787317276\n",
      "Epoch: 274, Training loss: 0.15000241994857788, Val loss: 0.15429992973804474\n",
      "Epoch: 275, Training loss: 0.1498727947473526, Val loss: 0.1542404443025589\n",
      "Epoch: 276, Training loss: 0.1497420072555542, Val loss: 0.15418055653572083\n",
      "Epoch: 277, Training loss: 0.14961013197898865, Val loss: 0.15412025153636932\n",
      "Epoch: 278, Training loss: 0.1494773030281067, Val loss: 0.15405946969985962\n",
      "Epoch: 279, Training loss: 0.14934326708316803, Val loss: 0.153998002409935\n",
      "Epoch: 280, Training loss: 0.14920781552791595, Val loss: 0.15393586456775665\n",
      "Epoch: 281, Training loss: 0.14907094836235046, Val loss: 0.1538730263710022\n",
      "Epoch: 282, Training loss: 0.14893263578414917, Val loss: 0.15380938351154327\n",
      "Epoch: 283, Training loss: 0.14879272878170013, Val loss: 0.15374498069286346\n",
      "Epoch: 284, Training loss: 0.14865121245384216, Val loss: 0.1536795198917389\n",
      "Epoch: 285, Training loss: 0.14850802719593048, Val loss: 0.1536131054162979\n",
      "Epoch: 286, Training loss: 0.1483631730079651, Val loss: 0.15354566276073456\n",
      "Epoch: 287, Training loss: 0.14821653068065643, Val loss: 0.15347681939601898\n",
      "Epoch: 288, Training loss: 0.14806810021400452, Val loss: 0.1534065306186676\n",
      "Epoch: 289, Training loss: 0.14791785180568695, Val loss: 0.15333491563796997\n",
      "Epoch: 290, Training loss: 0.14776566624641418, Val loss: 0.15326213836669922\n",
      "Epoch: 291, Training loss: 0.1476115733385086, Val loss: 0.15318810939788818\n",
      "Epoch: 292, Training loss: 0.14745552837848663, Val loss: 0.15311293303966522\n",
      "Epoch: 293, Training loss: 0.1472974568605423, Val loss: 0.15303663909435272\n",
      "Epoch: 294, Training loss: 0.14713738858699799, Val loss: 0.15295946598052979\n",
      "Epoch: 295, Training loss: 0.14697521924972534, Val loss: 0.15288110077381134\n",
      "Epoch: 296, Training loss: 0.146810844540596, Val loss: 0.15280139446258545\n",
      "Epoch: 297, Training loss: 0.14664427936077118, Val loss: 0.1527206152677536\n",
      "Epoch: 298, Training loss: 0.1464754194021225, Val loss: 0.15263834595680237\n",
      "Epoch: 299, Training loss: 0.14630430936813354, Val loss: 0.15255405008792877\n",
      "Epoch: 300, Training loss: 0.14613088965415955, Val loss: 0.15246737003326416\n",
      "Epoch: 301, Training loss: 0.1459549218416214, Val loss: 0.15237866342067719\n",
      "Epoch: 302, Training loss: 0.14577649533748627, Val loss: 0.15228813886642456\n",
      "Epoch: 303, Training loss: 0.14559558033943176, Val loss: 0.15219585597515106\n",
      "Epoch: 304, Training loss: 0.1454121321439743, Val loss: 0.1521022468805313\n",
      "Epoch: 305, Training loss: 0.14522601664066315, Val loss: 0.15200719237327576\n",
      "Epoch: 306, Training loss: 0.14503712952136993, Val loss: 0.15191102027893066\n",
      "Epoch: 307, Training loss: 0.14484544098377228, Val loss: 0.15181347727775574\n",
      "Epoch: 308, Training loss: 0.14465095102787018, Val loss: 0.15171444416046143\n",
      "Epoch: 309, Training loss: 0.14445361495018005, Val loss: 0.15161360800266266\n",
      "Epoch: 310, Training loss: 0.14425332844257355, Val loss: 0.15151092410087585\n",
      "Epoch: 311, Training loss: 0.14405019581317902, Val loss: 0.15140609443187714\n",
      "Epoch: 312, Training loss: 0.14384399354457855, Val loss: 0.15129944682121277\n",
      "Epoch: 313, Training loss: 0.14363479614257812, Val loss: 0.15119075775146484\n",
      "Epoch: 314, Training loss: 0.14342251420021057, Val loss: 0.15108029544353485\n",
      "Epoch: 315, Training loss: 0.1432071030139923, Val loss: 0.15096795558929443\n",
      "Epoch: 316, Training loss: 0.14298853278160095, Val loss: 0.15085354447364807\n",
      "Epoch: 317, Training loss: 0.14276692271232605, Val loss: 0.15073689818382263\n",
      "Epoch: 318, Training loss: 0.14254215359687805, Val loss: 0.1506178081035614\n",
      "Epoch: 319, Training loss: 0.14231406152248383, Val loss: 0.15049658715724945\n",
      "Epoch: 320, Training loss: 0.14208245277404785, Val loss: 0.1503729373216629\n",
      "Epoch: 321, Training loss: 0.14184705913066864, Val loss: 0.15024681389331818\n",
      "Epoch: 322, Training loss: 0.14160796999931335, Val loss: 0.15011751651763916\n",
      "Epoch: 323, Training loss: 0.14136500656604767, Val loss: 0.14998506009578705\n",
      "Epoch: 324, Training loss: 0.1411178708076477, Val loss: 0.14984966814517975\n",
      "Epoch: 325, Training loss: 0.14086660742759705, Val loss: 0.14971105754375458\n",
      "Epoch: 326, Training loss: 0.14061111211776733, Val loss: 0.14956963062286377\n",
      "Epoch: 327, Training loss: 0.1403512805700302, Val loss: 0.14942511916160583\n",
      "Epoch: 328, Training loss: 0.14008711278438568, Val loss: 0.1492777317762375\n",
      "Epoch: 329, Training loss: 0.13981838524341583, Val loss: 0.14912733435630798\n",
      "Epoch: 330, Training loss: 0.13954512774944305, Val loss: 0.14897368848323822\n",
      "Epoch: 331, Training loss: 0.13926714658737183, Val loss: 0.14881674945354462\n",
      "Epoch: 332, Training loss: 0.13898441195487976, Val loss: 0.14865611493587494\n",
      "Epoch: 333, Training loss: 0.13869664072990417, Val loss: 0.14849308133125305\n",
      "Epoch: 334, Training loss: 0.13840393722057343, Val loss: 0.14832736551761627\n",
      "Epoch: 335, Training loss: 0.13810624182224274, Val loss: 0.14815770089626312\n",
      "Epoch: 336, Training loss: 0.13780342042446136, Val loss: 0.14798447489738464\n",
      "Epoch: 337, Training loss: 0.13749544322490692, Val loss: 0.14780700206756592\n",
      "Epoch: 338, Training loss: 0.137182354927063, Val loss: 0.14762452244758606\n",
      "Epoch: 339, Training loss: 0.13686397671699524, Val loss: 0.14743651449680328\n",
      "Epoch: 340, Training loss: 0.136540025472641, Val loss: 0.14724434912204742\n",
      "Epoch: 341, Training loss: 0.1362103670835495, Val loss: 0.1470491588115692\n",
      "Epoch: 342, Training loss: 0.1358746737241745, Val loss: 0.14685185253620148\n",
      "Epoch: 343, Training loss: 0.13553297519683838, Val loss: 0.146651029586792\n",
      "Epoch: 344, Training loss: 0.13518492877483368, Val loss: 0.14644576609134674\n",
      "Epoch: 345, Training loss: 0.13483048975467682, Val loss: 0.14623606204986572\n",
      "Epoch: 346, Training loss: 0.1344694048166275, Val loss: 0.14602163434028625\n",
      "Epoch: 347, Training loss: 0.13410164415836334, Val loss: 0.14580248296260834\n",
      "Epoch: 348, Training loss: 0.13372701406478882, Val loss: 0.14557935297489166\n",
      "Epoch: 349, Training loss: 0.13334539532661438, Val loss: 0.14535334706306458\n",
      "Epoch: 350, Training loss: 0.13295674324035645, Val loss: 0.145123690366745\n",
      "Epoch: 351, Training loss: 0.1325608491897583, Val loss: 0.14489121735095978\n",
      "Epoch: 352, Training loss: 0.13215763866901398, Val loss: 0.14465366303920746\n",
      "Epoch: 353, Training loss: 0.1317470371723175, Val loss: 0.14441069960594177\n",
      "Epoch: 354, Training loss: 0.13132862746715546, Val loss: 0.14416365325450897\n",
      "Epoch: 355, Training loss: 0.1309025138616562, Val loss: 0.14391285181045532\n",
      "Epoch: 356, Training loss: 0.13046853244304657, Val loss: 0.14365734159946442\n",
      "Epoch: 357, Training loss: 0.13002651929855347, Val loss: 0.14339832961559296\n",
      "Epoch: 358, Training loss: 0.12957631051540375, Val loss: 0.14313623309135437\n",
      "Epoch: 359, Training loss: 0.12911804020404816, Val loss: 0.1428678184747696\n",
      "Epoch: 360, Training loss: 0.12865109741687775, Val loss: 0.14259320497512817\n",
      "Epoch: 361, Training loss: 0.12817580997943878, Val loss: 0.14231239259243011\n",
      "Epoch: 362, Training loss: 0.1276920586824417, Val loss: 0.142026886343956\n",
      "Epoch: 363, Training loss: 0.12719953060150146, Val loss: 0.14173947274684906\n",
      "Epoch: 364, Training loss: 0.12669812142848969, Val loss: 0.1414492428302765\n",
      "Epoch: 365, Training loss: 0.1261875033378601, Val loss: 0.14115364849567413\n",
      "Epoch: 366, Training loss: 0.12566760182380676, Val loss: 0.14085319638252258\n",
      "Epoch: 367, Training loss: 0.12513816356658936, Val loss: 0.14054656028747559\n",
      "Epoch: 368, Training loss: 0.1245991587638855, Val loss: 0.1402336061000824\n",
      "Epoch: 369, Training loss: 0.12405028939247131, Val loss: 0.13991579413414001\n",
      "Epoch: 370, Training loss: 0.12349151819944382, Val loss: 0.13959449529647827\n",
      "Epoch: 371, Training loss: 0.12292277067899704, Val loss: 0.13926804065704346\n",
      "Epoch: 372, Training loss: 0.12234396487474442, Val loss: 0.13893647491931915\n",
      "Epoch: 373, Training loss: 0.12175482511520386, Val loss: 0.1386013925075531\n",
      "Epoch: 374, Training loss: 0.12115532904863358, Val loss: 0.13826027512550354\n",
      "Epoch: 375, Training loss: 0.12054550647735596, Val loss: 0.13791167736053467\n",
      "Epoch: 376, Training loss: 0.11992520838975906, Val loss: 0.1375591903924942\n",
      "Epoch: 377, Training loss: 0.11929439753293991, Val loss: 0.13720542192459106\n",
      "Epoch: 378, Training loss: 0.11865313351154327, Val loss: 0.13684459030628204\n",
      "Epoch: 379, Training loss: 0.11800113320350647, Val loss: 0.1364803910255432\n",
      "Epoch: 380, Training loss: 0.11733855307102203, Val loss: 0.1361103653907776\n",
      "Epoch: 381, Training loss: 0.11666528880596161, Val loss: 0.1357375681400299\n",
      "Epoch: 382, Training loss: 0.11598128825426102, Val loss: 0.13535428047180176\n",
      "Epoch: 383, Training loss: 0.11528652161359787, Val loss: 0.13496622443199158\n",
      "Epoch: 384, Training loss: 0.11458103358745575, Val loss: 0.13457995653152466\n",
      "Epoch: 385, Training loss: 0.11386504024267197, Val loss: 0.13419511914253235\n",
      "Epoch: 386, Training loss: 0.11313782632350922, Val loss: 0.13380254805088043\n",
      "Epoch: 387, Training loss: 0.11239960044622421, Val loss: 0.13340140879154205\n",
      "Epoch: 388, Training loss: 0.11165022104978561, Val loss: 0.13299685716629028\n",
      "Epoch: 389, Training loss: 0.110859714448452, Val loss: 0.13258928060531616\n",
      "Epoch: 390, Training loss: 0.10279452055692673, Val loss: 0.13489939272403717\n",
      "Epoch: 391, Training loss: 0.1448933184146881, Val loss: 0.3086071014404297\n",
      "Epoch: 392, Training loss: 0.10016953200101852, Val loss: 0.14931008219718933\n",
      "Epoch: 393, Training loss: 0.11186327040195465, Val loss: 0.12304665893316269\n",
      "Epoch: 394, Training loss: 0.13195912539958954, Val loss: 0.13156363368034363\n",
      "Epoch: 395, Training loss: 0.1353592872619629, Val loss: 0.13363052904605865\n",
      "Epoch: 396, Training loss: 0.11868210136890411, Val loss: 0.12281069904565811\n",
      "Epoch: 397, Training loss: 0.08794574439525604, Val loss: 0.11843668669462204\n",
      "Epoch: 398, Training loss: 0.09343000501394272, Val loss: 0.23554743826389313\n",
      "Epoch: 399, Training loss: 0.10889636725187302, Val loss: 0.2705402672290802\n",
      "Epoch: 400, Training loss: 0.09174712747335434, Val loss: 0.23925836384296417\n",
      "Epoch: 401, Training loss: 0.079230897128582, Val loss: 0.20206479728221893\n",
      "Epoch: 402, Training loss: 0.08369414508342743, Val loss: 0.1926230490207672\n",
      "Epoch: 403, Training loss: 0.0893632248044014, Val loss: 0.19206681847572327\n",
      "Epoch: 404, Training loss: 0.08167443424463272, Val loss: 0.18806059658527374\n",
      "Epoch: 405, Training loss: 0.07057636976242065, Val loss: 0.1926671862602234\n",
      "Epoch: 406, Training loss: 0.07217845320701599, Val loss: 0.22265732288360596\n",
      "Epoch: 407, Training loss: 0.0743246003985405, Val loss: 0.23203270137310028\n",
      "Epoch: 408, Training loss: 0.06643202900886536, Val loss: 0.20058660209178925\n",
      "Epoch: 409, Training loss: 0.06916657835245132, Val loss: 0.18546681106090546\n",
      "Epoch: 410, Training loss: 0.07223385572433472, Val loss: 0.18297834694385529\n",
      "Epoch: 411, Training loss: 0.0695251077413559, Val loss: 0.18291011452674866\n",
      "Epoch: 412, Training loss: 0.06359771639108658, Val loss: 0.19095557928085327\n",
      "Epoch: 413, Training loss: 0.06410112977027893, Val loss: 0.21325147151947021\n",
      "Epoch: 414, Training loss: 0.06490413844585419, Val loss: 0.21910449862480164\n",
      "Epoch: 415, Training loss: 0.06057940050959587, Val loss: 0.19721819460391998\n",
      "Epoch: 416, Training loss: 0.06304071843624115, Val loss: 0.18363016843795776\n",
      "=== Early Stopping at epoch 397, best loss_val = 0.11843668669462204 ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "source_encoder = APPNP_Encoder(nfeat=8000,nhid=128,nclass=64,dropout=0.,with_bn=False)\n",
    "classifier = Classifier(nhids=[64,32,1],dropout=0.,with_bn=False)\n",
    "runner = SupRunner(data_src, label_src, data_tgt, label_tgt, source_encoder, classifier, device)\n",
    "runner.train(printing=True)\n",
    "runner.save('APPNP',outdir,savedir)\n",
    "runner.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPRGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start Training ===\n",
      "Epoch: 0, Training loss: 0.6926307678222656, Val loss: 0.6926302313804626\n",
      "Epoch: 1, Training loss: 0.6918883323669434, Val loss: 0.6918914318084717\n",
      "Epoch: 2, Training loss: 0.6903887391090393, Val loss: 0.6904011964797974\n",
      "Epoch: 3, Training loss: 0.6865012645721436, Val loss: 0.6865402460098267\n",
      "Epoch: 4, Training loss: 0.6784297823905945, Val loss: 0.6785210967063904\n",
      "Epoch: 5, Training loss: 0.663382351398468, Val loss: 0.663562536239624\n",
      "Epoch: 6, Training loss: 0.6378313899040222, Val loss: 0.6381336450576782\n",
      "Epoch: 7, Training loss: 0.5978903770446777, Val loss: 0.598304033279419\n",
      "Epoch: 8, Training loss: 0.5404398441314697, Val loss: 0.540817379951477\n",
      "Epoch: 9, Training loss: 0.46559029817581177, Val loss: 0.4654957056045532\n",
      "Epoch: 10, Training loss: 0.38089123368263245, Val loss: 0.37942418456077576\n",
      "Epoch: 11, Training loss: 0.3058987557888031, Val loss: 0.3016189932823181\n",
      "Epoch: 12, Training loss: 0.2690843939781189, Val loss: 0.26045286655426025\n",
      "Epoch: 13, Training loss: 0.2832713723182678, Val loss: 0.2698681950569153\n",
      "Epoch: 14, Training loss: 0.3218926787376404, Val loss: 0.30496421456336975\n",
      "Epoch: 15, Training loss: 0.34843912720680237, Val loss: 0.3300682306289673\n",
      "Epoch: 16, Training loss: 0.35023701190948486, Val loss: 0.3323180377483368\n",
      "Epoch: 17, Training loss: 0.33173859119415283, Val loss: 0.31558436155319214\n",
      "Epoch: 18, Training loss: 0.30285727977752686, Val loss: 0.289203405380249\n",
      "Epoch: 19, Training loss: 0.2738436460494995, Val loss: 0.2629658281803131\n",
      "Epoch: 20, Training loss: 0.2530375123023987, Val loss: 0.2448655515909195\n",
      "Epoch: 21, Training loss: 0.24482691287994385, Val loss: 0.23903784155845642\n",
      "Epoch: 22, Training loss: 0.24810832738876343, Val loss: 0.24419577419757843\n",
      "Epoch: 23, Training loss: 0.2570186257362366, Val loss: 0.2544177174568176\n",
      "Epoch: 24, Training loss: 0.26485908031463623, Val loss: 0.2630629241466522\n",
      "Epoch: 25, Training loss: 0.2673429846763611, Val loss: 0.2659465968608856\n",
      "Epoch: 26, Training loss: 0.2632940411567688, Val loss: 0.26197659969329834\n",
      "Epoch: 27, Training loss: 0.25395891070365906, Val loss: 0.2524486780166626\n",
      "Epoch: 28, Training loss: 0.24217675626277924, Val loss: 0.24023251235485077\n",
      "Epoch: 29, Training loss: 0.2313872128725052, Val loss: 0.22880671918392181\n",
      "Epoch: 30, Training loss: 0.2243916392326355, Val loss: 0.22104209661483765\n",
      "Epoch: 31, Training loss: 0.2222202569246292, Val loss: 0.2180713713169098\n",
      "Epoch: 32, Training loss: 0.22375079989433289, Val loss: 0.2188916802406311\n",
      "Epoch: 33, Training loss: 0.2264227271080017, Val loss: 0.22104863822460175\n",
      "Epoch: 34, Training loss: 0.22774779796600342, Val loss: 0.22211110591888428\n",
      "Epoch: 35, Training loss: 0.22642698884010315, Val loss: 0.22078607976436615\n",
      "Epoch: 36, Training loss: 0.2225659191608429, Val loss: 0.2171468883752823\n",
      "Epoch: 37, Training loss: 0.21730753779411316, Val loss: 0.21227966248989105\n",
      "Epoch: 38, Training loss: 0.21219725906848907, Val loss: 0.20766131579875946\n",
      "Epoch: 39, Training loss: 0.2085157185792923, Val loss: 0.20450295507907867\n",
      "Epoch: 40, Training loss: 0.20677343010902405, Val loss: 0.20325127243995667\n",
      "Epoch: 41, Training loss: 0.20664222538471222, Val loss: 0.20352093875408173\n",
      "Epoch: 42, Training loss: 0.20722874999046326, Val loss: 0.20438343286514282\n",
      "Epoch: 43, Training loss: 0.20754173398017883, Val loss: 0.20482875406742096\n",
      "Epoch: 44, Training loss: 0.2069324254989624, Val loss: 0.20420487225055695\n",
      "Epoch: 45, Training loss: 0.2053079754114151, Val loss: 0.20242811739444733\n",
      "Epoch: 46, Training loss: 0.20308329164981842, Val loss: 0.1999325007200241\n",
      "Epoch: 47, Training loss: 0.2009035348892212, Val loss: 0.19739295542240143\n",
      "Epoch: 48, Training loss: 0.19932183623313904, Val loss: 0.19540195167064667\n",
      "Epoch: 49, Training loss: 0.19856542348861694, Val loss: 0.19423258304595947\n",
      "Epoch: 50, Training loss: 0.1984846591949463, Val loss: 0.1937805563211441\n",
      "Epoch: 51, Training loss: 0.19868288934230804, Val loss: 0.19368848204612732\n",
      "Epoch: 52, Training loss: 0.19873294234275818, Val loss: 0.19355523586273193\n",
      "Epoch: 53, Training loss: 0.19837257266044617, Val loss: 0.19312924146652222\n",
      "Epoch: 54, Training loss: 0.19758129119873047, Val loss: 0.19238632917404175\n",
      "Epoch: 55, Training loss: 0.19655601680278778, Val loss: 0.19150637090206146\n",
      "Epoch: 56, Training loss: 0.19558098912239075, Val loss: 0.19074618816375732\n",
      "Epoch: 57, Training loss: 0.1948791891336441, Val loss: 0.19029484689235687\n",
      "Epoch: 58, Training loss: 0.19452357292175293, Val loss: 0.19019128382205963\n",
      "Epoch: 59, Training loss: 0.1944192796945572, Val loss: 0.1903107762336731\n",
      "Epoch: 60, Training loss: 0.19437140226364136, Val loss: 0.19043537974357605\n",
      "Epoch: 61, Training loss: 0.1941951960325241, Val loss: 0.19036439061164856\n",
      "Epoch: 62, Training loss: 0.19380316138267517, Val loss: 0.1900073140859604\n",
      "Epoch: 63, Training loss: 0.1932314783334732, Val loss: 0.18940585851669312\n",
      "Epoch: 64, Training loss: 0.1926012486219406, Val loss: 0.18869554996490479\n",
      "Epoch: 65, Training loss: 0.19204410910606384, Val loss: 0.18802917003631592\n",
      "Epoch: 66, Training loss: 0.1916312277317047, Val loss: 0.1875036656856537\n",
      "Epoch: 67, Training loss: 0.1913507580757141, Val loss: 0.18712951242923737\n",
      "Epoch: 68, Training loss: 0.19112628698349, Val loss: 0.1868494749069214\n",
      "Epoch: 69, Training loss: 0.19086866080760956, Val loss: 0.1865857094526291\n",
      "Epoch: 70, Training loss: 0.19051991403102875, Val loss: 0.18628434836864471\n",
      "Epoch: 71, Training loss: 0.1900774985551834, Val loss: 0.1859409660100937\n",
      "Epoch: 72, Training loss: 0.1895904392004013, Val loss: 0.18559283018112183\n",
      "Epoch: 73, Training loss: 0.18911977112293243, Val loss: 0.1852845400571823\n",
      "Epoch: 74, Training loss: 0.18870706856250763, Val loss: 0.18504029512405396\n",
      "Epoch: 75, Training loss: 0.18835903704166412, Val loss: 0.1848478466272354\n",
      "Epoch: 76, Training loss: 0.18804235756397247, Val loss: 0.1846591830253601\n",
      "Epoch: 77, Training loss: 0.18771103024482727, Val loss: 0.18441979587078094\n",
      "Epoch: 78, Training loss: 0.18733271956443787, Val loss: 0.18409231305122375\n",
      "Epoch: 79, Training loss: 0.18690231442451477, Val loss: 0.18367111682891846\n",
      "Epoch: 80, Training loss: 0.18644200265407562, Val loss: 0.1831866353750229\n",
      "Epoch: 81, Training loss: 0.18598809838294983, Val loss: 0.18268774449825287\n",
      "Epoch: 82, Training loss: 0.1855597198009491, Val loss: 0.1822030395269394\n",
      "Epoch: 83, Training loss: 0.18515533208847046, Val loss: 0.18174569308757782\n",
      "Epoch: 84, Training loss: 0.18475495278835297, Val loss: 0.1813080757856369\n",
      "Epoch: 85, Training loss: 0.1843358725309372, Val loss: 0.18087366223335266\n",
      "Epoch: 86, Training loss: 0.18388979136943817, Val loss: 0.18043121695518494\n",
      "Epoch: 87, Training loss: 0.18341955542564392, Val loss: 0.17997881770133972\n",
      "Epoch: 88, Training loss: 0.18293525278568268, Val loss: 0.17951802909374237\n",
      "Epoch: 89, Training loss: 0.1824481040239334, Val loss: 0.17904826998710632\n",
      "Epoch: 90, Training loss: 0.1819673329591751, Val loss: 0.17856477200984955\n",
      "Epoch: 91, Training loss: 0.1814921498298645, Val loss: 0.17806343734264374\n",
      "Epoch: 92, Training loss: 0.18101516366004944, Val loss: 0.17753826081752777\n",
      "Epoch: 93, Training loss: 0.1805332601070404, Val loss: 0.17698118090629578\n",
      "Epoch: 94, Training loss: 0.18004246056079865, Val loss: 0.17639122903347015\n",
      "Epoch: 95, Training loss: 0.17954163253307343, Val loss: 0.1757708340883255\n",
      "Epoch: 96, Training loss: 0.17903470993041992, Val loss: 0.17513246834278107\n",
      "Epoch: 97, Training loss: 0.17852044105529785, Val loss: 0.1744931936264038\n",
      "Epoch: 98, Training loss: 0.17800168693065643, Val loss: 0.17385198175907135\n",
      "Epoch: 99, Training loss: 0.17747315764427185, Val loss: 0.1732083261013031\n",
      "Epoch: 100, Training loss: 0.17693087458610535, Val loss: 0.1725607067346573\n",
      "Epoch: 101, Training loss: 0.17637328803539276, Val loss: 0.17190971970558167\n",
      "Epoch: 102, Training loss: 0.17580142617225647, Val loss: 0.17125600576400757\n",
      "Epoch: 103, Training loss: 0.17521703243255615, Val loss: 0.1705969125032425\n",
      "Epoch: 104, Training loss: 0.17462079226970673, Val loss: 0.16992652416229248\n",
      "Epoch: 105, Training loss: 0.17401207983493805, Val loss: 0.16923874616622925\n",
      "Epoch: 106, Training loss: 0.17338664829730988, Val loss: 0.1685398370027542\n",
      "Epoch: 107, Training loss: 0.1727491170167923, Val loss: 0.1678190529346466\n",
      "Epoch: 108, Training loss: 0.17209579050540924, Val loss: 0.1670769900083542\n",
      "Epoch: 109, Training loss: 0.1714300513267517, Val loss: 0.16632474958896637\n",
      "Epoch: 110, Training loss: 0.17075586318969727, Val loss: 0.1655614823102951\n",
      "Epoch: 111, Training loss: 0.17007698118686676, Val loss: 0.16479739546775818\n",
      "Epoch: 112, Training loss: 0.16939039528369904, Val loss: 0.16402915120124817\n",
      "Epoch: 113, Training loss: 0.1686909794807434, Val loss: 0.1632516086101532\n",
      "Epoch: 114, Training loss: 0.1679845154285431, Val loss: 0.16247478127479553\n",
      "Epoch: 115, Training loss: 0.16726775467395782, Val loss: 0.16168327629566193\n",
      "Epoch: 116, Training loss: 0.16653844714164734, Val loss: 0.16088852286338806\n",
      "Epoch: 117, Training loss: 0.16579753160476685, Val loss: 0.1601053923368454\n",
      "Epoch: 118, Training loss: 0.16503983736038208, Val loss: 0.15930353105068207\n",
      "Epoch: 119, Training loss: 0.16426675021648407, Val loss: 0.15848886966705322\n",
      "Epoch: 120, Training loss: 0.1634676605463028, Val loss: 0.15768040716648102\n",
      "Epoch: 121, Training loss: 0.16265282034873962, Val loss: 0.15685082972049713\n",
      "Epoch: 122, Training loss: 0.16181233525276184, Val loss: 0.15599147975444794\n",
      "Epoch: 123, Training loss: 0.16095112264156342, Val loss: 0.15511204302310944\n",
      "Epoch: 124, Training loss: 0.1600615233182907, Val loss: 0.15423288941383362\n",
      "Epoch: 125, Training loss: 0.1591455042362213, Val loss: 0.15334296226501465\n",
      "Epoch: 126, Training loss: 0.15820278227329254, Val loss: 0.15242426097393036\n",
      "Epoch: 127, Training loss: 0.15725505352020264, Val loss: 0.1514880508184433\n",
      "Epoch: 128, Training loss: 0.15628939867019653, Val loss: 0.15052835643291473\n",
      "Epoch: 129, Training loss: 0.15528957545757294, Val loss: 0.1495792716741562\n",
      "Epoch: 130, Training loss: 0.15426760911941528, Val loss: 0.14861051738262177\n",
      "Epoch: 131, Training loss: 0.15323396027088165, Val loss: 0.14763112366199493\n",
      "Epoch: 132, Training loss: 0.15219230949878693, Val loss: 0.14664579927921295\n",
      "Epoch: 133, Training loss: 0.15113337337970734, Val loss: 0.14565256237983704\n",
      "Epoch: 134, Training loss: 0.15004466474056244, Val loss: 0.1446441411972046\n",
      "Epoch: 135, Training loss: 0.14893431961536407, Val loss: 0.143609419465065\n",
      "Epoch: 136, Training loss: 0.14778129756450653, Val loss: 0.14259688556194305\n",
      "Epoch: 137, Training loss: 0.14660435914993286, Val loss: 0.1415274292230606\n",
      "Epoch: 138, Training loss: 0.14541761577129364, Val loss: 0.14040341973304749\n",
      "Epoch: 139, Training loss: 0.14424389600753784, Val loss: 0.13923978805541992\n",
      "Epoch: 140, Training loss: 0.1430492103099823, Val loss: 0.13807152211666107\n",
      "Epoch: 141, Training loss: 0.1418076604604721, Val loss: 0.1369207352399826\n",
      "Epoch: 142, Training loss: 0.14053528010845184, Val loss: 0.13572189211845398\n",
      "Epoch: 143, Training loss: 0.1392141878604889, Val loss: 0.13445885479450226\n",
      "Epoch: 144, Training loss: 0.13784372806549072, Val loss: 0.13313975930213928\n",
      "Epoch: 145, Training loss: 0.13641992211341858, Val loss: 0.13180691003799438\n",
      "Epoch: 146, Training loss: 0.13494274020195007, Val loss: 0.13042502105236053\n",
      "Epoch: 147, Training loss: 0.13340947031974792, Val loss: 0.12898731231689453\n",
      "Epoch: 148, Training loss: 0.13180416822433472, Val loss: 0.12749534845352173\n",
      "Epoch: 149, Training loss: 0.13013052940368652, Val loss: 0.1259431540966034\n",
      "Epoch: 150, Training loss: 0.12838152050971985, Val loss: 0.12431032955646515\n",
      "Epoch: 151, Training loss: 0.12654706835746765, Val loss: 0.12260392308235168\n",
      "Epoch: 152, Training loss: 0.12461229413747787, Val loss: 0.120819590985775\n",
      "Epoch: 153, Training loss: 0.12258054316043854, Val loss: 0.11892291903495789\n",
      "Epoch: 154, Training loss: 0.12044007331132889, Val loss: 0.11694503575563431\n",
      "Epoch: 155, Training loss: 0.11818556487560272, Val loss: 0.11486460268497467\n",
      "Epoch: 156, Training loss: 0.11579694598913193, Val loss: 0.11270017921924591\n",
      "Epoch: 157, Training loss: 0.11325595527887344, Val loss: 0.11035632342100143\n",
      "Epoch: 158, Training loss: 0.11040504276752472, Val loss: 0.10768333077430725\n",
      "Epoch: 159, Training loss: 0.10425285995006561, Val loss: 0.10046551376581192\n",
      "Epoch: 160, Training loss: 0.09093182533979416, Val loss: 0.08721356093883514\n",
      "Epoch: 161, Training loss: 0.08662835508584976, Val loss: 0.08443212509155273\n",
      "Epoch: 162, Training loss: 0.0825682207942009, Val loss: 0.08411800861358643\n",
      "Epoch: 163, Training loss: 0.07894595712423325, Val loss: 0.0828850045800209\n",
      "Epoch: 164, Training loss: 0.07598642259836197, Val loss: 0.08206455409526825\n",
      "Epoch: 165, Training loss: 0.07222537696361542, Val loss: 0.08093007653951645\n",
      "Epoch: 166, Training loss: 0.06835485994815826, Val loss: 0.08094804733991623\n",
      "Epoch: 167, Training loss: 0.0649954304099083, Val loss: 0.08144497126340866\n",
      "Epoch: 168, Training loss: 0.06159793958067894, Val loss: 0.08038017898797989\n",
      "Epoch: 169, Training loss: 0.05828750878572464, Val loss: 0.0791003406047821\n",
      "Epoch: 170, Training loss: 0.0550324022769928, Val loss: 0.16132138669490814\n",
      "Epoch: 171, Training loss: 0.052322953939437866, Val loss: 0.15947777032852173\n",
      "Epoch: 172, Training loss: 0.04924289137125015, Val loss: 0.1573750525712967\n",
      "Epoch: 173, Training loss: 0.04503890126943588, Val loss: 0.15659977495670319\n",
      "Epoch: 174, Training loss: 0.04168401658535004, Val loss: 0.15419459342956543\n",
      "Epoch: 175, Training loss: 0.03921301290392876, Val loss: 0.15505793690681458\n",
      "Epoch: 176, Training loss: 0.03644828870892525, Val loss: 0.15549063682556152\n",
      "Epoch: 177, Training loss: 0.033107008785009384, Val loss: 0.15446355938911438\n",
      "Epoch: 178, Training loss: 0.029256759211421013, Val loss: 0.15290085971355438\n",
      "Epoch: 179, Training loss: 0.02604070119559765, Val loss: 0.15212875604629517\n",
      "Epoch: 180, Training loss: 0.0238175131380558, Val loss: 0.1527477502822876\n",
      "Epoch: 181, Training loss: 0.021641040220856667, Val loss: 0.1532265692949295\n",
      "Epoch: 182, Training loss: 0.018718283623456955, Val loss: 0.15365245938301086\n",
      "Epoch: 183, Training loss: 0.015789801254868507, Val loss: 0.15328358113765717\n",
      "Epoch: 184, Training loss: 0.01317502185702324, Val loss: 0.1540452092885971\n",
      "Epoch: 185, Training loss: 0.011230265721678734, Val loss: 0.15577009320259094\n",
      "Epoch: 186, Training loss: 0.012554144486784935, Val loss: 0.15751498937606812\n",
      "Epoch: 187, Training loss: 0.008182407356798649, Val loss: 0.157770037651062\n",
      "Epoch: 188, Training loss: 0.0071940235793590546, Val loss: 0.16074436902999878\n",
      "=== Early Stopping at epoch 169, best loss_val = 0.0791003406047821 ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "source_encoder = GPRGNN_Encoder(in_channels=8000,hidden_channels=128,out_channels=64,dropout=0.)\n",
    "classifier = Classifier(nhids=[64,32,1],dropout=0.,with_bn=False)\n",
    "runner = SupRunner(data_src, label_src, data_tgt, label_tgt, source_encoder, classifier, device)\n",
    "runner.train(printing=True)\n",
    "runner.save('GPRGNN',outdir,savedir)\n",
    "runner.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Start Training ===\n",
      "Epoch: 0, Training loss: 0.6927663683891296, Val loss: 0.6927660703659058\n",
      "Epoch: 1, Training loss: 0.6914620995521545, Val loss: 0.6914538741111755\n",
      "Epoch: 2, Training loss: 0.5835624933242798, Val loss: 0.583482027053833\n",
      "Epoch: 3, Training loss: 0.36190953850746155, Val loss: 0.3616024851799011\n",
      "Epoch: 4, Training loss: 0.2099938541650772, Val loss: 0.2089279145002365\n",
      "Epoch: 5, Training loss: 0.3252859115600586, Val loss: 0.32333388924598694\n",
      "Epoch: 6, Training loss: 0.31648191809654236, Val loss: 0.3145853877067566\n",
      "Epoch: 7, Training loss: 0.2339831292629242, Val loss: 0.2326568365097046\n",
      "Epoch: 8, Training loss: 0.21907654404640198, Val loss: 0.21837657690048218\n",
      "Epoch: 9, Training loss: 0.2574850022792816, Val loss: 0.2569471001625061\n",
      "Epoch: 10, Training loss: 0.2299930453300476, Val loss: 0.2292887419462204\n",
      "Epoch: 11, Training loss: 0.2054136097431183, Val loss: 0.20425403118133545\n",
      "Epoch: 12, Training loss: 0.2099515199661255, Val loss: 0.20870430767536163\n",
      "Epoch: 13, Training loss: 0.2098444253206253, Val loss: 0.20844322443008423\n",
      "Epoch: 14, Training loss: 0.21090847253799438, Val loss: 0.20987091958522797\n",
      "Epoch: 15, Training loss: 0.20941667258739471, Val loss: 0.20840609073638916\n",
      "Epoch: 16, Training loss: 0.20481489598751068, Val loss: 0.2037600576877594\n",
      "Epoch: 17, Training loss: 0.20622937381267548, Val loss: 0.20520977675914764\n",
      "Epoch: 18, Training loss: 0.2067934274673462, Val loss: 0.20602262020111084\n",
      "Epoch: 19, Training loss: 0.2052050232887268, Val loss: 0.20436422526836395\n",
      "Epoch: 20, Training loss: 0.20330896973609924, Val loss: 0.20210973918437958\n",
      "Epoch: 21, Training loss: 0.20369137823581696, Val loss: 0.20261602103710175\n",
      "Epoch: 22, Training loss: 0.20374716818332672, Val loss: 0.20260342955589294\n",
      "Epoch: 23, Training loss: 0.20443931221961975, Val loss: 0.2035493552684784\n",
      "Epoch: 24, Training loss: 0.2031065672636032, Val loss: 0.20235110819339752\n",
      "Epoch: 25, Training loss: 0.20236939191818237, Val loss: 0.20139728486537933\n",
      "Epoch: 26, Training loss: 0.19978629052639008, Val loss: 0.19868814945220947\n",
      "Epoch: 27, Training loss: 0.19951683282852173, Val loss: 0.198225736618042\n",
      "Epoch: 28, Training loss: 0.20052213966846466, Val loss: 0.19942647218704224\n",
      "Epoch: 29, Training loss: 0.19952481985092163, Val loss: 0.19869983196258545\n",
      "Epoch: 30, Training loss: 0.19867919385433197, Val loss: 0.19746676087379456\n",
      "Epoch: 31, Training loss: 0.19643910229206085, Val loss: 0.19505636394023895\n",
      "Epoch: 32, Training loss: 0.1918061375617981, Val loss: 0.190717875957489\n",
      "Epoch: 33, Training loss: 0.19067025184631348, Val loss: 0.18931062519550323\n",
      "Epoch: 34, Training loss: 0.1942487508058548, Val loss: 0.1928245574235916\n",
      "Epoch: 35, Training loss: 0.19907158613204956, Val loss: 0.1979539394378662\n",
      "Epoch: 36, Training loss: 0.1898115873336792, Val loss: 0.1887943297624588\n",
      "Epoch: 37, Training loss: 0.18559035658836365, Val loss: 0.18514201045036316\n",
      "Epoch: 38, Training loss: 0.19758519530296326, Val loss: 0.19590291380882263\n",
      "Epoch: 39, Training loss: 0.18950888514518738, Val loss: 0.18777912855148315\n",
      "Epoch: 40, Training loss: 0.18557800352573395, Val loss: 0.184668630361557\n",
      "Epoch: 41, Training loss: 0.17580366134643555, Val loss: 0.17343676090240479\n",
      "Epoch: 42, Training loss: 0.1787211000919342, Val loss: 0.17639166116714478\n",
      "Epoch: 43, Training loss: 0.1749991923570633, Val loss: 0.171309232711792\n",
      "Epoch: 44, Training loss: 0.17462539672851562, Val loss: 0.172959104180336\n",
      "Epoch: 45, Training loss: 0.17599447071552277, Val loss: 0.1753145009279251\n",
      "Epoch: 46, Training loss: 0.17014195024967194, Val loss: 0.16766849160194397\n",
      "Epoch: 47, Training loss: 0.1754906326532364, Val loss: 0.17159698903560638\n",
      "Epoch: 48, Training loss: 0.17238645255565643, Val loss: 0.16678553819656372\n",
      "Epoch: 49, Training loss: 0.16311167180538177, Val loss: 0.1562132090330124\n",
      "Epoch: 50, Training loss: 0.2170342355966568, Val loss: 0.21429578959941864\n",
      "Epoch: 51, Training loss: 0.14185062050819397, Val loss: 0.13488227128982544\n",
      "Epoch: 52, Training loss: 0.19437886774539948, Val loss: 0.18902158737182617\n",
      "Epoch: 53, Training loss: 0.14002591371536255, Val loss: 0.13449792563915253\n",
      "Epoch: 54, Training loss: 0.1410352736711502, Val loss: 0.13883663713932037\n",
      "Epoch: 55, Training loss: 0.13570378720760345, Val loss: 0.1379641443490982\n",
      "Epoch: 56, Training loss: 0.11725731939077377, Val loss: 0.12026896327733994\n",
      "Epoch: 57, Training loss: 0.16887231171131134, Val loss: 0.17360806465148926\n",
      "Epoch: 58, Training loss: 0.2116670161485672, Val loss: 0.19404636323451996\n",
      "Epoch: 59, Training loss: 0.21153075993061066, Val loss: 0.2079736292362213\n",
      "Epoch: 60, Training loss: 0.1641254872083664, Val loss: 0.16199177503585815\n",
      "Epoch: 61, Training loss: 0.2079780548810959, Val loss: 0.20364663004875183\n",
      "Epoch: 62, Training loss: 0.1959397941827774, Val loss: 0.190286323428154\n",
      "Epoch: 63, Training loss: 0.2052200436592102, Val loss: 0.19305023550987244\n",
      "Epoch: 64, Training loss: 0.17824777960777283, Val loss: 0.16773714125156403\n",
      "Epoch: 65, Training loss: 0.17026551067829132, Val loss: 0.16267655789852142\n",
      "Epoch: 66, Training loss: 0.1715913861989975, Val loss: 0.16634942591190338\n",
      "Epoch: 67, Training loss: 0.1631350964307785, Val loss: 0.15874500572681427\n",
      "Epoch: 68, Training loss: 0.15633845329284668, Val loss: 0.15238605439662933\n",
      "Epoch: 69, Training loss: 0.152772918343544, Val loss: 0.15075963735580444\n",
      "Epoch: 70, Training loss: 0.17808760702610016, Val loss: 0.1711646020412445\n",
      "Epoch: 71, Training loss: 0.14193286001682281, Val loss: 0.13391460478305817\n",
      "Epoch: 72, Training loss: 0.2214815765619278, Val loss: 0.18812373280525208\n",
      "Epoch: 73, Training loss: 0.16999293863773346, Val loss: 0.12941376864910126\n",
      "Epoch: 74, Training loss: 0.19995315372943878, Val loss: 0.15208666026592255\n",
      "Epoch: 75, Training loss: 0.17521199584007263, Val loss: 0.1285402476787567\n",
      "=== Early Stopping at epoch 56, best loss_val = 0.12026896327733994 ===\n",
      "=== DONE ===\n"
     ]
    }
   ],
   "source": [
    "source_encoder = GAT_Encoder(nhids=[8000,128,16],dropout=0.,heads=8,output_heads=4,with_bn=False)\n",
    "classifier = Classifier(nhids=[64,32,1],dropout=0.,with_bn=False)\n",
    "runner = SupRunner(data_src, label_src, data_tgt, label_tgt, source_encoder, classifier, device)\n",
    "runner.train(printing=True)\n",
    "runner.save('GAT',outdir,savedir)\n",
    "runner.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_H2R = 'output/H2R_sup'\n",
    "out_R2H = 'output/R2H_sup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist_H2R = []\n",
    "for file in os.listdir(out_H2R):\n",
    "    modelname = file.split('_')[0]\n",
    "    df = pd.read_csv(os.path.join(out_H2R,file))\n",
    "    df['model'] = modelname\n",
    "    dflist_H2R.append(df)\n",
    "df_H2R = pd.concat(dflist_H2R,axis=0)\n",
    "df_H2R = df_H2R.rename(columns={df_H2R.columns[0]: 'partition'})\n",
    "df_H2R.to_excel(os.path.join(out_H2R,'H2R.xlsx'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dflist_R2H = []\n",
    "for file in os.listdir(out_R2H):\n",
    "    modelname = file.split('_')[0]\n",
    "    df = pd.read_csv(os.path.join(out_R2H,file))\n",
    "    df['model'] = modelname\n",
    "    dflist_R2H.append(df)\n",
    "df_R2H = pd.concat(dflist_R2H,axis=0)\n",
    "df_R2H = df_R2H.rename(columns={df_R2H.columns[0]: 'partition'})\n",
    "df_R2H.to_excel(os.path.join(out_R2H,'R2H.xlsx'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Xuanlin2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
